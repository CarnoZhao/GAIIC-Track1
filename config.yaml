model_name: &model_name hfl/chinese-roberta-wwm-ext

model:
  type: MultiHeadVisualBert
  model_name: *model_name
  num_classes: 2
  drop_rate: 0.5
  num_heads: 13
loss:
  type: CrossEntropyLoss
  ignore_index: -1
data:
  type: JointData
  fold: 0
  batch_size: 256
  model_name: *model_name
  # >>>>
  # position holder for augmentation args
  # <<<<
train:
  type: adam
  learning_rate: 1e-4
  num_epochs: 25
  weight_decay: 1e-3
  swa: false
metric:
  type: binary_accuracy
name: roberta
seed: 0
version: v1
